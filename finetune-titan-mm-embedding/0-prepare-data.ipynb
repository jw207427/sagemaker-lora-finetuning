{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "51d00759-23ea-401f-b7cb-91ce5fcd7342",
   "metadata": {},
   "source": [
    "### > Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "42e711eb-858a-4a40-897e-b7f03674a1cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /home/sagemaker-user/.config/sagemaker/config.yaml\n",
      "Created new directory: data\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import uuid\n",
    "import shutil\n",
    "\n",
    "from helper import (\n",
    "    get_text_response,\n",
    "    calc_total_cost,\n",
    "    load_jsonl,\n",
    "    random_split,\n",
    "    download_file_from_s3,\n",
    "    _encode\n",
    ")\n",
    "import sagemaker\n",
    "import boto3\n",
    "\n",
    "\n",
    "sess = sagemaker.Session()\n",
    "bucket = sess.default_bucket() # Set a default S3 bucket\n",
    "\n",
    "s3_prefix =\"titan-finetuning/multi-modal-embedding\"\n",
    "\n",
    "# corpus data directory\n",
    "directory_path = \"data\"\n",
    "\n",
    "# Check if the directory exists\n",
    "if os.path.exists(directory_path):\n",
    "    shutil.rmtree(directory_path)\n",
    "\n",
    "# Create the new directory\n",
    "os.mkdir(directory_path)\n",
    "print(f\"Created new directory: {directory_path}\")\n",
    "\n",
    "# initialize S3 clinet\n",
    "s3 = boto3.client('s3')\n",
    "\n",
    "#Anthropic Calude 3 Sonnet pricing\n",
    "llm_price = {'input_tokens': 0.003/1000, 'output_tokens': 0.015/1000}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1d80df43-7d0e-4b73-8e06-01f1b5535d0f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data_file = \"../image-generator/image_data.jsonl\"\n",
    "data = load_jsonl(data_file)\n",
    "train_corpus, valid_corpus = random_split(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dd56566-a6f2-4fc0-8ace-1018a656545a",
   "metadata": {},
   "source": [
    "### Generate synthetic queries\n",
    "\n",
    "Now, we use Claude 3 from Amazon Bedrock to generate questions using each text chunk in the corpus as context.\n",
    "\n",
    "Each pair of (generated question, caption, and image) becomes a datapoint in the finetuning dataset (either for training or evaluation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a1d8e6a9-0027-4742-907d-8f073bdc4475",
   "metadata": {},
   "outputs": [],
   "source": [
    "template=\"\"\"\n",
    "Your task is to generate relevant search queries based on this image. Your goal is to generate {QUERY_NUMBER} search\n",
    "queries that a user might enter into a search engine to find images similar to the one described in\n",
    "the caption.\n",
    "\n",
    "To generate the search queries, carefully analyze the image. Identify the key\n",
    "objects, scenes, actions, and concepts present in the description. Then, construct search queries\n",
    "that capture these elements in a concise and relevant manner.\n",
    "\n",
    "When formulating the search queries, consider the following:\n",
    "\n",
    "- Generate relevant keywords and phrases from the image\n",
    "- Vary the length and specificity of the queries (e.g., some queries can be broad, while others can\n",
    "be more specific)\n",
    "- Include queries that focus on different aspects of the image (objects, actions, scenes, etc.)\n",
    "- Ensure that the queries are grammatically correct and make sense in the context of a search engine\n",
    "\n",
    "Once you have generated the 20 search queries, format them as a JSON list, with each query as a\n",
    "separate string element within the list.\n",
    "\n",
    "Your output should look like this:\n",
    "\n",
    "[\n",
    "    \"query 1\",\n",
    "    \"query 2\",\n",
    "    ...\n",
    "    \"query 20\"\n",
    "]\n",
    "\n",
    "Please note that you should only provide the JSON list of search queries. Do not include any additional text or explanations.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2310e6b8-b78e-454b-8c5b-ee975bc80ce5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def generate_queries(corpus, template, number_of_qs=2):\n",
    "\n",
    "    cost = 0\n",
    "    queries = dict()\n",
    "    mapping = dict()\n",
    "    bedrock_data = []\n",
    "    \n",
    "    for node_id, data in tqdm(corpus.items()):\n",
    "        image = download_file_from_s3(data[\"image-ref\"])\n",
    "        image_base64 = _encode(image)\n",
    "        \n",
    "        prompt = template.replace(\"{QUERY_NUMBER}\", str(number_of_qs))\n",
    "    \n",
    "        try:\n",
    "            model_response = get_text_response(image_base64=image_base64, text_query=prompt)\n",
    "            questions = json.loads(model_response[\"content\"][0]['text'])\n",
    "            tokens = model_response['usage']\n",
    "        except json.JSONDecodeError as e:\n",
    "            raise \"Unable to generate valid JSON, please try again...\"\n",
    "    \n",
    "        for q in questions:\n",
    "            q_id = str(uuid.uuid4())\n",
    "            queries[q_id] = q\n",
    "            mapping[q_id] = [node_id]\n",
    "            bedrock_data.append({\n",
    "                \"image-ref\":data[\"image-ref\"],\n",
    "                \"caption\": q\n",
    "            })\n",
    "            \n",
    "        cost += calc_total_cost(llm_price, tokens)\n",
    "    \n",
    "    print(f\"Estimated cost: ${cost:.2f}\")\n",
    "    return queries, mapping, bedrock_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "551d8903-a16c-480e-9579-ccd35988aa26",
   "metadata": {},
   "source": [
    "### > building training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "617e0f7d-5071-4647-a07e-a81974ef0684",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 64/64 [06:25<00:00,  6.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated cost: $0.52\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "train_queries, train_maping, bedrock_train = generate_queries(train_corpus, template, number_of_qs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1243e8ce-79fb-448d-afa2-603686151875",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 17/17 [01:47<00:00,  6.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated cost: $0.14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "valid_queries, valid_maping, bedrock_valid = generate_queries(valid_corpus, template, number_of_qs=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27cde629-ff98-4984-8103-0fb79a5c1ef8",
   "metadata": {},
   "source": [
    "### > create the final training and validation dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2432fed4-3b0f-4936-8883-b9b22c76ad9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_path = './data/train.json'\n",
    "valid_data_path = './data/valid.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4fdb6697-05f5-46ea-b7fa-a00cad93c4d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = {\n",
    "    'queries': train_queries,\n",
    "    'corpus': train_corpus,\n",
    "    'relevant_docs': train_maping,\n",
    "}\n",
    "\n",
    "val_dataset = {\n",
    "    'queries': valid_queries,\n",
    "    'corpus': valid_corpus,\n",
    "    'relevant_docs': valid_maping,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8bebe5e5-e604-4316-8afe-2d3a03e47390",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(train_data_path, 'w+') as f:\n",
    "    json.dump(train_dataset, f)\n",
    "\n",
    "with open(valid_data_path, 'w+') as f:\n",
    "    json.dump(val_dataset, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94e0fec4-a712-40a3-8f61-7c8563b43d7e",
   "metadata": {},
   "source": [
    "### > Upload bedrock training data to S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3b7915df-b688-459a-bdf6-953db2884565",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JSONL file 'data/training-embedding-data.jsonl' created successfully.\n"
     ]
    }
   ],
   "source": [
    "# Path to the JSONL file\n",
    "jsonl_file_path = \"data/training-embedding-data.jsonl\"\n",
    "\n",
    "try:\n",
    "    with open(jsonl_file_path, \"w\", encoding=\"utf-8\") as file:\n",
    "        for item in bedrock_train:\n",
    "            json_line = json.dumps(item, ensure_ascii=False)\n",
    "            file.write(json_line + \"\\n\")\n",
    "    print(f\"JSONL file '{jsonl_file_path}' created successfully.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error creating JSONL file: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d583da23-ebcb-40ab-9d34-2f1aefcecbdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image data file 'data/training-embedding-data.jsonl' uploaded to S3\n",
      "Stored 'train_jsonl_path' (str)\n"
     ]
    }
   ],
   "source": [
    "metadata_key = f\"{s3_prefix}/metadata/{jsonl_file_path}\"\n",
    "# upload file to S3\n",
    "s3.upload_file(jsonl_file_path, bucket, metadata_key)\n",
    "print(f\"Image data file '{jsonl_file_path}' uploaded to S3\")\n",
    "\n",
    "train_jsonl_path = f\"s3://{bucket}/{metadata_key}\"\n",
    "%store train_jsonl_path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd7a9e56-cf6d-4b19-b41d-b68b4438b650",
   "metadata": {},
   "source": [
    "### > Upload bedrock validation data to S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "46022553-9f27-4e43-abc7-b033919f8f7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JSONL file 'data/validation-embedding-data.jsonl' created successfully.\n"
     ]
    }
   ],
   "source": [
    "# Path to the JSONL file\n",
    "jsonl_file_path = \"data/validation-embedding-data.jsonl\"\n",
    "\n",
    "try:\n",
    "    with open(jsonl_file_path, \"w\", encoding=\"utf-8\") as file:\n",
    "        for item in bedrock_valid:\n",
    "            json_line = json.dumps(item, ensure_ascii=False)\n",
    "            file.write(json_line + \"\\n\")\n",
    "    print(f\"JSONL file '{jsonl_file_path}' created successfully.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error creating JSONL file: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6f246109-4338-45b6-bbb2-00c4102dd916",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image data file 'data/validation-embedding-data.jsonl' uploaded to S3\n",
      "Stored 'valid_jsonl_path' (str)\n"
     ]
    }
   ],
   "source": [
    "metadata_key = f\"{s3_prefix}/metadata/{jsonl_file_path}\"\n",
    "# upload file to S3\n",
    "s3.upload_file(jsonl_file_path, bucket, metadata_key)\n",
    "print(f\"Image data file '{jsonl_file_path}' uploaded to S3\")\n",
    "\n",
    "valid_jsonl_path = f\"s3://{bucket}/{metadata_key}\"\n",
    "%store valid_jsonl_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "345be0dd-86f1-49cb-9b44-36f5a5fa2e35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 'train_data_path' (str)\n",
      "Stored 'valid_data_path' (str)\n"
     ]
    }
   ],
   "source": [
    "%store train_data_path\n",
    "%store valid_data_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1e15beb-fed9-4ee4-b312-5d94540ee657",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7ae87e9-ef7c-4177-8f6a-ee50ce5c903d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
